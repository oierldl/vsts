# __vSTS__: Visual Semantic Textual Similarity

We present present Visual Semantic Textual Similarity (vSTS) that
extends the Semantic Textual Similarity to the visual modality, a task
and dataset which allows to study whether better sentence
representations can be built when having access to the corresponding
images, in contrast with having access to the text alone.

The vSTS dataset aims to become a standard benchmark to test the
contribution of visual information when evaluating the similarity of
sentences and the quality of multimodal representations, allowing to
test the complementarity of visual and textual information for
improved language understanding.


For more details check out our ECAI paper [__Evaluating Multimodal Representations on Visual Semantic Textual Similarity__](https://oierldl.github.io/vsts/paper/ECAI_2020___vSTS.pdf), and [website](https://oierldl.github.io/vsts/).

The code and models presented in our [paper](https://oierldl.github.io/vsts/paper/ECAI_2020___vSTS.pdf), see our repository at: CREATE A NEW REPO FOR THIS

Full dataset can be downloaded [here](http://ixa2.si.ehu.eus/~jibloleo/visual_sts.v2.0.tar.gz)


## Data description and format

TBD